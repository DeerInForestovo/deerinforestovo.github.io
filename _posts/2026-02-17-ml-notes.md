---
layout: post
title: Notes for CMU-18661 Machine Learning
subtitle: CMU-18661 机器学习课程笔记
categories: article
top: 2
banner:
    image: assets/images/banners/article-ml-notes.jpg
excerpt: "CMU-18661 机器学习课程笔记"
image_source: "https://www.pixiv.net/artworks/139508420"
toc: false
---

# Lecture 1: Intro

- Homeworks (40%): Both math and programming problems
- Miniexams (15%): Two during the semester
- Midterm Exam (15%): Linear and Logistic Regression, Naive Bayes, SVMs (subject to change)
- Final Exam (25%): Everything else (Nearest Neighbors, Neural Networks, Decision Trees, Boosting, Clustering, PCA, etc.), plus pre-midterm topics (subject to change)
- Gradescope Quizzes (5%): Short multiple-choice question quizzes conducted in random (possibly all) lectures to encourage class attendance and attention. We will take the best 10 quiz scores.

# Lecture 2: MLE & MAP

Pending...

# Lecture 3-4: Linear Regression

Pending...

# Lecture 5: Overfitting

Pending...

# Lecture 6: Naive Bayes

Naive Bayes:

$$
P(y|\mathbf{x}) = \frac{P(\mathbf{x}|y)P(y)}{P(\mathbf{x})}
$$

$$
\begin{aligned}
P(\mathbf{X}=\mathbf{x}, Y=c)&=P(Y=c)P(X=x|Y=c)\\
&=P(Y=c)\prod_{k=1}^K P(\text{word}_k|Y=c)^{x_k}\\
&=\pi_c\prod_{k=1}^K \theta_{c,k}^{x_k},
\end{aligned}
$$

where $\pi_c = P(Y = c)$ is the prior probability of class $c$, $x_k$ is the number of occurrences of the $k$-th word, and $\theta_{c,k} = P(\text{word}_k |Y = c)$ is the weight of the $k$-th word for the $c$-th class.

Training Data:

$$
\mathcal{D}=\{(\mathbf{x}_n,y_n)\}_{n=1}^N=\{(\{x_{n,k}\}_{k=1}^K,y_n)\}_{n=1}^N
$$

